<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
        body, html { margin: 0; height: 100%; font-family: Arial, sans-serif; background: black; color: white }
        #status-gif { width: 100%; height: auto; }
    </style>
</head>
<body>
    <img id="status-gif" src="normal-state.gif" alt="Status"> <!-- Replace with actual path to normal-state.gif -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            const statusGif = document.getElementById('status-gif');
            let recognition;
            let isRecording = false;
            let wakeWordDetected = false;
            let messageHistory = [{ role: "system", content: "System message content here" }];
            let currentMessage = '';
            let silenceTimer = null;

            function initializeSpeechRecognition() {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = true;
                recognition.lang = 'en-US';
                recognition.interimResults = true;
                recognition.onresult = handleRecognitionResult;
                recognition.onend = restartSpeechRecognition;
                recognition.start();
                resetSilenceTimer();
                console.log("Speech recognition initialized.");
            }

            function restartSpeechRecognition() {
                if (!wakeWordDetected && !recognition.continuous) {
                    recognition.start();
                    console.log("Speech recognition restarted.");
                }
                resetSilenceTimer();
            }

function handleRecognitionResult(event) {
    let interimTranscript = '';

    for (let i = event.resultIndex; i < event.results.length; ++i) {
        if (event.results[i][0]) {
            let transcript = event.results[i][0].transcript.trim().replace(/,/g, "");
            interimTranscript += transcript;

            if (event.results[i].isFinal) {
                console.log("Heard:", transcript);
                resetSilenceTimer(); // Reset timer on any final speech detected

                if (!wakeWordDetected && interimTranscript.toLowerCase().includes("hey hub")) {
                    wakeWordDetected = true;
                    isRecording = true;
                    let parts = interimTranscript.toLowerCase().split("hey hub");
                    currentMessage = parts.length > 1 ? parts[1].trim() : "";
                    changeGif('wake-word-detected.gif'); // Replace with actual path
                    console.log("Wake word detected. Switching to listening mode.");
                } else if (wakeWordDetected) {
                    currentMessage += ' ' + transcript;
                    changeGif('word-spoken.gif'); // Replace with actual path
                    if (isRecording) {
                        setTimeout(sendInputToAPI, 2000); // Send input after 2 seconds of pause
                    }
                }
            }
        }
    }
}

function sendInputToAPI() {
    isRecording = false;
    changeGif('sending-to-api.gif'); // Replace with actual path
    console.log("Sending to API:", currentMessage);
    messageHistory.push({ role: "user", content: currentMessage });
    if (messageHistory.length > 50) {
        messageHistory = messageHistory.slice(-50);
    }

    let token = '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'; // Correct API key
    fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({ model: "gpt-3.5-turbo", messages: messageHistory })
    })
    .then(response => response.json())
    .then(data => {
        let aiResponse = data.choices[0].message.content;
        console.log("API response:", aiResponse);
        messageHistory.push({ role: "assistant", content: aiResponse });
        receiveAPIResponse(aiResponse);
    })
    .catch(error => console.error('Error:', error));
}


            function receiveAPIResponse(response) {
                changeGif('responding.gif'); // Replace with actual path
                speak(response);
                resetState();
            }

            function speak(text) {
                console.log("Speaking:", text);
                const utterance = new SpeechSynthesisUtterance(text);
                window.speechSynthesis.speak(utterance);
            }

            function resetState() {
                wakeWordDetected = false;
                currentMessage = '';
                setTimeout(() => {
                    changeGif('normal-state.gif'); // Replace with actual path
                }, 5000); // Reset state after 5 seconds
            }

            function resetSilenceTimer() {
                clearTimeout(silenceTimer);
                silenceTimer = setTimeout(() => {
                    if (!isRecording) {
                        recognition.stop();
                        recognition.start();
                        changeGif('normal-state.gif'); // Replace with actual path
                        console.log("No speech detected. Resetting.");
                    }
                }, 30000); // 30 seconds of silence
            }

            function changeGif(gifName) {
                statusGif.src = gifName;
                console.log("Gif changed to:", gifName);
            }

            initializeSpeechRecognition();
        });
    </script>
</body>
</html>
